{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import lightgbm\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "import copy\n",
    "from preprocessing import data_loader\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from models.model_factory import ModelFactory\n",
    "from main import load_config, run_kfold_training, run_train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data for: LUAD, BRCA\n",
      "Loading cached aligned expression and mutation data from:\n",
      "C:\\Users\\KerenYlab.MEDICINE\\OneDrive - Technion\\Asaf\\Expression_to_Mutation\\mutation_prediction\\cache\\expression_aligned_BRCA-LUAD.pkl C:\\Users\\KerenYlab.MEDICINE\\OneDrive - Technion\\Asaf\\Expression_to_Mutation\\mutation_prediction\\cache\\mutation_aligned_BRCA-LUAD.pkl\n"
     ]
    }
   ],
   "source": [
    "# Initialize components\n",
    "selected_cohorts = [\"LUAD\", \"BRCA\"]\n",
    "data_load = data_loader.TCGADataLoader(use_cache=True)\n",
    "\n",
    "# Load and preprocess data for the selected cohorts\n",
    "print(f\"Loading and preprocessing data for: {', '.join(selected_cohorts)}\")\n",
    "expression_data, mutation_data = data_load.preprocess_data(cancer_types=selected_cohorts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config_path = Path(\"../config/config.yaml\")\n",
    "config = load_config(config_path)\n",
    "\n",
    "X_log = np.log1p(expression_data)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_log)\n",
    "\n",
    "\n",
    "cohort_suffix = \"-\".join(selected_cohorts)\n",
    "results_root = Path(\"../results/notebook_multitask_test\") / cohort_suffix\n",
    "results_root.mkdir(parents=True, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KerenYlab.MEDICINE\\AppData\\Local\\anaconda3\\envs\\DNA_to_RNA\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train/test evaluation complete.\n",
      "                    f1   roc_auc  accuracy\n",
      "multitask_nn  0.304076  0.794594  0.723675\n",
      "âœ… Train/test evaluation complete.\n",
      "                  f1  roc_auc  accuracy\n",
      "neural_net  0.301886  0.78241  0.845082\n",
      "[LightGBM] [Info] Number of positive: 52, number of negative: 976\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.964833 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.050584 -> initscore=-2.932219\n",
      "[LightGBM] [Info] Start training from score -2.932219\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 95, number of negative: 933\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.935503 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.092412 -> initscore=-2.284528\n",
      "[LightGBM] [Info] Start training from score -2.284528\n",
      "[LightGBM] [Info] Number of positive: 83, number of negative: 945\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.999212 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080739 -> initscore=-2.432344\n",
      "[LightGBM] [Info] Start training from score -2.432344\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 73, number of negative: 955\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.009205 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.071012 -> initscore=-2.571252\n",
      "[LightGBM] [Info] Start training from score -2.571252\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 58, number of negative: 970\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.078558 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.056420 -> initscore=-2.816853\n",
      "[LightGBM] [Info] Start training from score -2.816853\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 73, number of negative: 955\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.863675 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.071012 -> initscore=-2.571252\n",
      "[LightGBM] [Info] Start training from score -2.571252\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 135, number of negative: 893\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.830067 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.131323 -> initscore=-1.889312\n",
      "[LightGBM] [Info] Start training from score -1.889312\n",
      "[LightGBM] [Info] Number of positive: 103, number of negative: 925\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.896824 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100195 -> initscore=-2.195065\n",
      "[LightGBM] [Info] Start training from score -2.195065\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 52, number of negative: 976\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.702302 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.050584 -> initscore=-2.932219\n",
      "[LightGBM] [Info] Start training from score -2.932219\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 68, number of negative: 960\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.822399 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066148 -> initscore=-2.647426\n",
      "[LightGBM] [Info] Start training from score -2.647426\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 70, number of negative: 958\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.798935 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.068093 -> initscore=-2.616353\n",
      "[LightGBM] [Info] Start training from score -2.616353\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 59, number of negative: 969\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.637034 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.057393 -> initscore=-2.798727\n",
      "[LightGBM] [Info] Start training from score -2.798727\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 61, number of negative: 967\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.925646 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.059339 -> initscore=-2.763325\n",
      "[LightGBM] [Info] Start training from score -2.763325\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 119, number of negative: 909\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.826775 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.115759 -> initscore=-2.033222\n",
      "[LightGBM] [Info] Start training from score -2.033222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 77, number of negative: 951\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.193725 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074903 -> initscore=-2.513709\n",
      "[LightGBM] [Info] Start training from score -2.513709\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 69, number of negative: 959\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.802371 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.067121 -> initscore=-2.631785\n",
      "[LightGBM] [Info] Start training from score -2.631785\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 55, number of negative: 973\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.051106 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.053502 -> initscore=-2.873051\n",
      "[LightGBM] [Info] Start training from score -2.873051\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 80, number of negative: 948\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.967078 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.077821 -> initscore=-2.472328\n",
      "[LightGBM] [Info] Start training from score -2.472328\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 72, number of negative: 956\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.675991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070039 -> initscore=-2.586092\n",
      "[LightGBM] [Info] Start training from score -2.586092\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 73, number of negative: 955\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.921409 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.071012 -> initscore=-2.571252\n",
      "[LightGBM] [Info] Start training from score -2.571252\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 73, number of negative: 955\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.196923 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.071012 -> initscore=-2.571252\n",
      "[LightGBM] [Info] Start training from score -2.571252\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 52, number of negative: 976\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.062970 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.050584 -> initscore=-2.932219\n",
      "[LightGBM] [Info] Start training from score -2.932219\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 57, number of negative: 971\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.362472 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.055447 -> initscore=-2.835275\n",
      "[LightGBM] [Info] Start training from score -2.835275\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 64, number of negative: 964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.789822 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.062257 -> initscore=-2.712208\n",
      "[LightGBM] [Info] Start training from score -2.712208\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 51, number of negative: 977\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.149510 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.049611 -> initscore=-2.952661\n",
      "[LightGBM] [Info] Start training from score -2.952661\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cohort_suffix = \"-\".join(selected_cohorts)\n",
    "results_root = Path(\"../results/notebook_multitask_test\") / cohort_suffix\n",
    "results_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "models_to_compare = [\"multitask_nn\", \"neural_net\", \"lightgbm\"]\n",
    "comparison_rows = []\n",
    "\n",
    "for model_name in models_to_compare:\n",
    "    config_variant = copy.deepcopy(config)\n",
    "    config_variant['model']['name'] = model_name\n",
    "\n",
    "    factory = ModelFactory()\n",
    "    model = factory.get_model(\n",
    "        model_name=model_name,\n",
    "        input_size=X_scaled.shape[1],\n",
    "        output_size=mutation_data.shape[1],\n",
    "        config=config_variant,\n",
    "    )\n",
    "\n",
    "    model_dir = results_root / model_name\n",
    "    metrics = run_train_test_split(\n",
    "        model=model,\n",
    "        X=X_scaled,\n",
    "        Y=mutation_data,\n",
    "        test_size=config_variant['preprocessing']['test_size'],\n",
    "        output_dir=model_dir,\n",
    "        config_meta={\n",
    "            'config': config_variant,\n",
    "            'cohorts': selected_cohorts,\n",
    "            'model_name': model_name,\n",
    "        },\n",
    "        random_state=config_variant['preprocessing']['random_state'],\n",
    "        label=model_name,\n",
    "    )\n",
    "\n",
    "    mean_metrics = metrics.mean()\n",
    "    comparison_rows.append({'model': model_name, **mean_metrics.to_dict()})\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_rows).set_index('model')\n",
    "comparison_df.sort_values('f1', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training multitask model on full dataset...\n",
      "Hidden layers: [2048, 1536, 1024]\n",
      "Head layers: [] (simple linear)\n",
      "Output directory: ..\\results\\notebook_multitask_test\\LUAD-BRCA\\head_weights_extraction\n",
      "\n",
      "ðŸ§¬ Training multitask model on full dataset...\n",
      "   Genes: 129\n",
      "   Hidden layers: [2048, 1536, 1024]\n",
      "   Head layers: Simple linear\n",
      "   Training on full dataset (no validation split)...\n",
      "   Extracting head weights...\n",
      "   Model and weights saved to: ..\\results\\notebook_multitask_test\\LUAD-BRCA\\head_weights_extraction\n",
      "âœ… Training complete!\n",
      "   Head weight matrix shape: torch.Size([129, 1024])\n",
      "\n",
      "============================================================\n",
      "Head Weights Extraction Complete!\n",
      "============================================================\n",
      "Weight matrix shape: torch.Size([129, 1024])\n",
      "Number of genes: 129\n",
      "\n",
      "First 10 genes:\n",
      "  - ZNF831\n",
      "  - OBSCN\n",
      "  - ABCA13\n",
      "  - ASTN1\n",
      "  - SVEP1\n",
      "  - ASXL3\n",
      "  - SPTA1\n",
      "  - FAT3\n",
      "  - FAT1\n",
      "  - FAT4\n",
      "\n",
      "Top 10 genes by weight norm:\n",
      "        gene  weight_norm\n",
      "95     KEAP1     1.429592\n",
      "52     STK11     1.385130\n",
      "106   PIK3CA     1.366320\n",
      "93      CDH1     1.357066\n",
      "114    GATA3     1.353912\n",
      "65     NRXN1     1.340952\n",
      "57    MAP3K1     1.340415\n",
      "31      KRAS     1.334810\n",
      "50   COL12A1     1.324962\n",
      "118    TAF1L     1.324384\n",
      "\n",
      "Gene weight norms saved to: ..\\results\\notebook_multitask_test\\LUAD-BRCA\\head_weights_extraction\\gene_weight_norms.csv\n"
     ]
    }
   ],
   "source": [
    "# Train multitask model on full dataset and extract head weights\n",
    "from training.extract_weights import train_and_extract_head_weights\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare data (using the same preprocessing as before)\n",
    "X_log = np.log1p(expression_data)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_log)\n",
    "\n",
    "# Configure model with specified architecture\n",
    "config_weights = copy.deepcopy(config)\n",
    "config_weights['model']['name'] = 'multitask_nn'\n",
    "config_weights['model']['multitask_nn'].update({\n",
    "    'hidden_layers': [2048, 1536, 1024],\n",
    "    'head_layers': [],  # Simple linear output\n",
    "    'dropout_rate': 0.2,\n",
    "    'learning_rate': 0.0005,\n",
    "    'epochs': 60,\n",
    "    'batch_size': 128,\n",
    "})\n",
    "\n",
    "# Create output directory\n",
    "weights_output_dir = results_root / \"head_weights_extraction\"\n",
    "weights_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Training multitask model on full dataset...\")\n",
    "print(f\"Hidden layers: {config_weights['model']['multitask_nn']['hidden_layers']}\")\n",
    "print(f\"Head layers: {config_weights['model']['multitask_nn']['head_layers']} (simple linear)\")\n",
    "print(f\"Output directory: {weights_output_dir}\")\n",
    "\n",
    "# Train and extract head weights\n",
    "factory = ModelFactory()\n",
    "head_weights = train_and_extract_head_weights(\n",
    "    model_factory=factory,\n",
    "    X=X_scaled,\n",
    "    Y=mutation_data,\n",
    "    config=config_weights,\n",
    "    hidden_layers=[2048, 1536, 1024],\n",
    "    head_layers=None,  # Simple linear output\n",
    "    output_dir=weights_output_dir,\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Head Weights Extraction Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Weight matrix shape: {head_weights['weight_matrix_shape']}\")\n",
    "print(f\"Number of genes: {len(head_weights['gene_names'])}\")\n",
    "print(f\"\\nFirst 10 genes:\")\n",
    "for gene in head_weights['gene_names'][:10]:\n",
    "    print(f\"  - {gene}\")\n",
    "\n",
    "# Create a DataFrame with gene names and their weight vector norms\n",
    "weight_matrix = head_weights['all_weights'].cpu().numpy()\n",
    "weight_norms = np.linalg.norm(weight_matrix, axis=1)\n",
    "weights_df = pd.DataFrame({\n",
    "    'gene': head_weights['gene_names'],\n",
    "    'weight_norm': weight_norms,\n",
    "})\n",
    "weights_df = weights_df.sort_values('weight_norm', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 10 genes by weight norm:\")\n",
    "print(weights_df.head(10))\n",
    "\n",
    "# Save weights DataFrame\n",
    "weights_df.to_csv(weights_output_dir / \"gene_weight_norms.csv\", index=False)\n",
    "print(f\"\\nGene weight norms saved to: {weights_output_dir / 'gene_weight_norms.csv'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run gene ablation analysis\n",
    "from interpretation.ablation import run_gene_ablation_analysis\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare data (using the same preprocessing as before)\n",
    "X_log = np.log1p(expression_data)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_log)\n",
    "\n",
    "# Configure model for ablation (should be multitask_nn)\n",
    "config_ablation = copy.deepcopy(config)\n",
    "config_ablation['model']['name'] = 'multitask_nn'\n",
    "# Use the same architecture as the head weights extraction for consistency\n",
    "config_ablation['model']['multitask_nn'].update({\n",
    "    'hidden_layers': [2048, 1536, 1024],\n",
    "    'head_layers': [],  # Simple linear output\n",
    "    'dropout_rate': 0.2,\n",
    "    'learning_rate': 0.0005,\n",
    "    'epochs': 60,\n",
    "    'batch_size': 128,\n",
    "})\n",
    "\n",
    "# Create output directory\n",
    "ablation_output_dir = results_root / \"gene_ablation_analysis\"\n",
    "ablation_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Running Gene Ablation Analysis\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Evaluation mode: train_test\")\n",
    "print(f\"Number of genes: {mutation_data.shape[1]}\")\n",
    "print(f\"This will train {mutation_data.shape[1]} models (one per removed gene)\")\n",
    "print(f\"Output directory: {ablation_output_dir}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run ablation analysis\n",
    "factory = ModelFactory()\n",
    "metric_matrices = run_gene_ablation_analysis(\n",
    "    model_factory=factory,\n",
    "    X=X_scaled,\n",
    "    Y=mutation_data,\n",
    "    config=config_ablation,\n",
    "    output_dir=results_root,\n",
    "    eval_mode='train_test',  # Use 'kfold' for cross-validation\n",
    "    test_size=config_ablation['preprocessing']['test_size'],\n",
    "    random_state=config_ablation['preprocessing']['random_state'],\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Ablation Analysis Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Generated {len(metric_matrices)} metric matrices:\")\n",
    "for metric_name in metric_matrices.keys():\n",
    "    print(f\"  - {metric_name}\")\n",
    "\n",
    "# Display a sample matrix (F1 score)\n",
    "if 'f1' in metric_matrices:\n",
    "    print(\"\\nF1 Score Ablation Matrix (removed_gene Ã— evaluated_gene):\")\n",
    "    print(\"Rows = gene removed, Columns = gene evaluated\")\n",
    "    print(metric_matrices['f1'].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize and explore ablation results\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load ablation matrices from saved files\n",
    "ablation_dir = results_root / \"gene_ablation_analysis\"\n",
    "print(f\"Loading ablation matrices from: {ablation_dir}\")\n",
    "\n",
    "# Load all metric matrices\n",
    "metric_matrices = {}\n",
    "metric_files = list(ablation_dir.glob(\"ablation_matrix_*.csv\"))\n",
    "for metric_file in metric_files:\n",
    "    metric_name = metric_file.stem.replace(\"ablation_matrix_\", \"\")\n",
    "    metric_matrices[metric_name] = pd.read_csv(metric_file, index_col=0)\n",
    "\n",
    "print(f\"\\nLoaded {len(metric_matrices)} metric matrices:\")\n",
    "for metric_name in metric_matrices.keys():\n",
    "    print(f\"  - {metric_name}\")\n",
    "\n",
    "# Visualize F1 score matrix as heatmap\n",
    "if 'f1' in metric_matrices:\n",
    "    f1_matrix = metric_matrices['f1']\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(16, 14))\n",
    "    sns.heatmap(\n",
    "        f1_matrix,\n",
    "        cmap='viridis',\n",
    "        center=f1_matrix.values[~np.isnan(f1_matrix.values)].mean() if not f1_matrix.isna().all().all() else 0,\n",
    "        annot=False,  # Set to True to show values (may be cluttered with many genes)\n",
    "        fmt='.3f',\n",
    "        cbar_kws={'label': 'F1 Score'},\n",
    "        square=False,\n",
    "        linewidths=0.5,\n",
    "        linecolor='gray'\n",
    "    )\n",
    "    plt.title('Gene Ablation Analysis - F1 Score Matrix\\n(Removed Gene Ã— Evaluated Gene)', fontsize=14, pad=20)\n",
    "    plt.xlabel('Evaluated Gene', fontsize=12)\n",
    "    plt.ylabel('Removed Gene', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ablation_dir / \"f1_ablation_heatmap.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze which gene removals have the biggest impact\n",
    "    # Calculate mean F1 for each row (when that gene was removed)\n",
    "    row_means = f1_matrix.mean(axis=1, skipna=True).sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 genes whose removal has the LEAST impact (highest mean F1 when removed):\")\n",
    "    print(row_means.head(10))\n",
    "    \n",
    "    print(\"\\nTop 10 genes whose removal has the MOST impact (lowest mean F1 when removed):\")\n",
    "    print(row_means.tail(10))\n",
    "    \n",
    "    # Calculate mean F1 for each column (how well that gene is predicted when others are removed)\n",
    "    col_means = f1_matrix.mean(axis=0, skipna=True).sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 genes that are predicted BEST when others are removed:\")\n",
    "    print(col_means.head(10))\n",
    "    \n",
    "    print(\"\\nTop 10 genes that are predicted WORST when others are removed:\")\n",
    "    print(col_means.tail(10))\n",
    "\n",
    "# Visualize ROC-AUC matrix\n",
    "if 'roc_auc' in metric_matrices:\n",
    "    roc_matrix = metric_matrices['roc_auc']\n",
    "    \n",
    "    plt.figure(figsize=(16, 14))\n",
    "    sns.heatmap(\n",
    "        roc_matrix,\n",
    "        cmap='plasma',\n",
    "        center=0.5,\n",
    "        annot=False,\n",
    "        fmt='.3f',\n",
    "        cbar_kws={'label': 'ROC-AUC'},\n",
    "        square=False,\n",
    "        linewidths=0.5,\n",
    "        linecolor='gray'\n",
    "    )\n",
    "    plt.title('Gene Ablation Analysis - ROC-AUC Matrix\\n(Removed Gene Ã— Evaluated Gene)', fontsize=14, pad=20)\n",
    "    plt.xlabel('Evaluated Gene', fontsize=12)\n",
    "    plt.ylabel('Removed Gene', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ablation_dir / \"roc_auc_ablation_heatmap.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\nAll visualizations saved to: {ablation_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "\n",
    "# Define search grid for multitask NN\n",
    "hidden_layer_sets = [\n",
    "    [2048, 1536, 1024, 512],\n",
    "    [2048, 1024, 512],\n",
    "    [1536, 1024, 512],\n",
    "    [1024, 768, 512],\n",
    "    [1024, 512, 256],\n",
    "]\n",
    "head_layer_sets = [\n",
    "    [512, 256],\n",
    "    [256, 128],\n",
    "    [512],\n",
    "    [256],\n",
    "    [128]\n",
    "    []\n",
    "]\n",
    "dropout_rates = [0.2, 0.3]\n",
    "learning_rates = [5e-4, 1e-4]\n",
    "\n",
    "search_params = []\n",
    "for hidden_layers, head_layers, dropout_rate, lr in itertools.product(\n",
    "    hidden_layer_sets, head_layer_sets, dropout_rates, learning_rates\n",
    "):\n",
    "    # Skip redundant configuration where head equals main width and dropout high\n",
    "    search_params.append(\n",
    "        {\n",
    "            'hidden_layers': hidden_layers,\n",
    "            'head_layers': head_layers,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'learning_rate': lr,\n",
    "        }\n",
    "    )\n",
    "\n",
    "grid_root = results_root / \"multitask_grid\"\n",
    "grid_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "grid_rows = []\n",
    "\n",
    "for run_idx, params in enumerate(search_params, start=1):\n",
    "    cfg = copy.deepcopy(config)\n",
    "    cfg['model']['name'] = 'multitask_nn'\n",
    "    cfg['model']['multitask_nn'].update(params)\n",
    "\n",
    "    factory = ModelFactory()\n",
    "    model = factory.get_model(\n",
    "        model_name='multitask_nn',\n",
    "        input_size=X_small.shape[1],\n",
    "        output_size=Y_small.shape[1],\n",
    "        config=cfg,\n",
    "    )\n",
    "\n",
    "    run_dir = grid_root / f\"run_{run_idx:02d}\"\n",
    "    metrics = run_train_test_split(\n",
    "        model=model,\n",
    "        X=X_small,\n",
    "        Y=Y_small,\n",
    "        test_size=cfg['preprocessing']['test_size'],\n",
    "        output_dir=run_dir,\n",
    "        config_meta={\n",
    "            'selected_features': selected_features,\n",
    "            'config': cfg,\n",
    "            'cohorts': selected_cohorts,\n",
    "            'search_params': params,\n",
    "        },\n",
    "        random_state=cfg['preprocessing']['random_state'],\n",
    "        label=f\"multitask_nn_run_{run_idx:02d}\",\n",
    "    )\n",
    "\n",
    "    mean_metrics = metrics.mean()\n",
    "    grid_rows.append({\n",
    "        'run': run_idx,\n",
    "        **params,\n",
    "        **mean_metrics.to_dict(),\n",
    "    })\n",
    "\n",
    "grid_df = pd.DataFrame(grid_rows).set_index('run')\n",
    "grid_df.sort_values('f1', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DNA_to_RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
