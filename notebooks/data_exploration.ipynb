{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import lightgbm\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "import copy\n",
    "from preprocessing import data_loader\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from models.model_factory import ModelFactory\n",
    "from main import load_config, run_kfold_training, run_train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data for: LUAD, BRCA\n",
      "Loading cached aligned expression and mutation data from:\n",
      "C:\\Users\\KerenYlab.MEDICINE\\OneDrive - Technion\\Asaf\\Expression_to_Mutation\\mutation_prediction\\cache\\expression_aligned_BRCA-LUAD.pkl C:\\Users\\KerenYlab.MEDICINE\\OneDrive - Technion\\Asaf\\Expression_to_Mutation\\mutation_prediction\\cache\\mutation_aligned_BRCA-LUAD.pkl\n"
     ]
    }
   ],
   "source": [
    "# Initialize components\n",
    "selected_cohorts = [\"LUAD\", \"BRCA\"]\n",
    "data_load = data_loader.TCGADataLoader(use_cache=True)\n",
    "\n",
    "# Load and preprocess data for the selected cohorts\n",
    "print(f\"Loading and preprocessing data for: {', '.join(selected_cohorts)}\")\n",
    "expression_data, mutation_data = data_load.preprocess_data(cancer_types=selected_cohorts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KerenYlab.MEDICINE\\AppData\\Local\\anaconda3\\envs\\DNA_to_RNA\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train/test evaluation complete.\n",
      "                    f1   roc_auc  accuracy\n",
      "multitask_nn  0.304076  0.794594  0.723675\n",
      "✅ Train/test evaluation complete.\n",
      "                  f1  roc_auc  accuracy\n",
      "neural_net  0.301886  0.78241  0.845082\n",
      "[LightGBM] [Info] Number of positive: 52, number of negative: 976\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.964833 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.050584 -> initscore=-2.932219\n",
      "[LightGBM] [Info] Start training from score -2.932219\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 95, number of negative: 933\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.935503 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.092412 -> initscore=-2.284528\n",
      "[LightGBM] [Info] Start training from score -2.284528\n",
      "[LightGBM] [Info] Number of positive: 83, number of negative: 945\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.999212 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080739 -> initscore=-2.432344\n",
      "[LightGBM] [Info] Start training from score -2.432344\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 73, number of negative: 955\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.009205 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.071012 -> initscore=-2.571252\n",
      "[LightGBM] [Info] Start training from score -2.571252\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 58, number of negative: 970\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.078558 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.056420 -> initscore=-2.816853\n",
      "[LightGBM] [Info] Start training from score -2.816853\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 73, number of negative: 955\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.863675 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.071012 -> initscore=-2.571252\n",
      "[LightGBM] [Info] Start training from score -2.571252\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 135, number of negative: 893\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.830067 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.131323 -> initscore=-1.889312\n",
      "[LightGBM] [Info] Start training from score -1.889312\n",
      "[LightGBM] [Info] Number of positive: 103, number of negative: 925\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.896824 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100195 -> initscore=-2.195065\n",
      "[LightGBM] [Info] Start training from score -2.195065\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 52, number of negative: 976\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.702302 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.050584 -> initscore=-2.932219\n",
      "[LightGBM] [Info] Start training from score -2.932219\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 68, number of negative: 960\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.822399 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066148 -> initscore=-2.647426\n",
      "[LightGBM] [Info] Start training from score -2.647426\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 70, number of negative: 958\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.798935 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.068093 -> initscore=-2.616353\n",
      "[LightGBM] [Info] Start training from score -2.616353\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 59, number of negative: 969\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.637034 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.057393 -> initscore=-2.798727\n",
      "[LightGBM] [Info] Start training from score -2.798727\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 61, number of negative: 967\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.925646 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.059339 -> initscore=-2.763325\n",
      "[LightGBM] [Info] Start training from score -2.763325\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 119, number of negative: 909\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.826775 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.115759 -> initscore=-2.033222\n",
      "[LightGBM] [Info] Start training from score -2.033222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 77, number of negative: 951\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.193725 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074903 -> initscore=-2.513709\n",
      "[LightGBM] [Info] Start training from score -2.513709\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 69, number of negative: 959\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.802371 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.067121 -> initscore=-2.631785\n",
      "[LightGBM] [Info] Start training from score -2.631785\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 55, number of negative: 973\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.051106 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.053502 -> initscore=-2.873051\n",
      "[LightGBM] [Info] Start training from score -2.873051\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 80, number of negative: 948\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.967078 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.077821 -> initscore=-2.472328\n",
      "[LightGBM] [Info] Start training from score -2.472328\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 72, number of negative: 956\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.675991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070039 -> initscore=-2.586092\n",
      "[LightGBM] [Info] Start training from score -2.586092\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 73, number of negative: 955\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.921409 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.071012 -> initscore=-2.571252\n",
      "[LightGBM] [Info] Start training from score -2.571252\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 73, number of negative: 955\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.196923 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.071012 -> initscore=-2.571252\n",
      "[LightGBM] [Info] Start training from score -2.571252\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 52, number of negative: 976\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.062970 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.050584 -> initscore=-2.932219\n",
      "[LightGBM] [Info] Start training from score -2.932219\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 57, number of negative: 971\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.362472 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.055447 -> initscore=-2.835275\n",
      "[LightGBM] [Info] Start training from score -2.835275\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 64, number of negative: 964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.789822 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.062257 -> initscore=-2.712208\n",
      "[LightGBM] [Info] Start training from score -2.712208\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 51, number of negative: 977\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.149510 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3800537\n",
      "[LightGBM] [Info] Number of data points in the train set: 1028, number of used features: 19210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.049611 -> initscore=-2.952661\n",
      "[LightGBM] [Info] Start training from score -2.952661\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config_path = Path(\"../config/config.yaml\")\n",
    "config = load_config(config_path)\n",
    "\n",
    "X_log = np.log1p(expression_data)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_log)\n",
    "\n",
    "cohort_suffix = \"-\".join(selected_cohorts)\n",
    "results_root = Path(\"../results/notebook_multitask_test\") / cohort_suffix\n",
    "results_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "models_to_compare = [\"multitask_nn\", \"neural_net\", \"lightgbm\"]\n",
    "comparison_rows = []\n",
    "\n",
    "for model_name in models_to_compare:\n",
    "    config_variant = copy.deepcopy(config)\n",
    "    config_variant['model']['name'] = model_name\n",
    "\n",
    "    factory = ModelFactory()\n",
    "    model = factory.get_model(\n",
    "        model_name=model_name,\n",
    "        input_size=X_scaled.shape[1],\n",
    "        output_size=mutation_data.shape[1],\n",
    "        config=config_variant,\n",
    "    )\n",
    "\n",
    "    model_dir = results_root / model_name\n",
    "    metrics = run_train_test_split(\n",
    "        model=model,\n",
    "        X=X_scaled,\n",
    "        Y=mutation_data,\n",
    "        test_size=config_variant['preprocessing']['test_size'],\n",
    "        output_dir=model_dir,\n",
    "        config_meta={\n",
    "            'config': config_variant,\n",
    "            'cohorts': selected_cohorts,\n",
    "            'model_name': model_name,\n",
    "        },\n",
    "        random_state=config_variant['preprocessing']['random_state'],\n",
    "        label=model_name,\n",
    "    )\n",
    "\n",
    "    mean_metrics = metrics.mean()\n",
    "    comparison_rows.append({'model': model_name, **mean_metrics.to_dict()})\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_rows).set_index('model')\n",
    "comparison_df.sort_values('f1', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "\n",
    "# Define search grid for multitask NN\n",
    "hidden_layer_sets = [\n",
    "    [2048, 1536, 1024, 512],\n",
    "    [2048, 1024, 512],\n",
    "    [1536, 1024, 512],\n",
    "    [1024, 768, 512],\n",
    "    [1024, 512, 256],\n",
    "]\n",
    "head_layer_sets = [\n",
    "    [512, 256],\n",
    "    [256, 128],\n",
    "    [512],\n",
    "    [256],\n",
    "    [128]\n",
    "    []\n",
    "]\n",
    "dropout_rates = [0.2, 0.3]\n",
    "learning_rates = [5e-4, 1e-4]\n",
    "\n",
    "search_params = []\n",
    "for hidden_layers, head_layers, dropout_rate, lr in itertools.product(\n",
    "    hidden_layer_sets, head_layer_sets, dropout_rates, learning_rates\n",
    "):\n",
    "    # Skip redundant configuration where head equals main width and dropout high\n",
    "    search_params.append(\n",
    "        {\n",
    "            'hidden_layers': hidden_layers,\n",
    "            'head_layers': head_layers,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'learning_rate': lr,\n",
    "        }\n",
    "    )\n",
    "\n",
    "grid_root = results_root / \"multitask_grid\"\n",
    "grid_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "grid_rows = []\n",
    "\n",
    "for run_idx, params in enumerate(search_params, start=1):\n",
    "    cfg = copy.deepcopy(config)\n",
    "    cfg['model']['name'] = 'multitask_nn'\n",
    "    cfg['model']['multitask_nn'].update(params)\n",
    "\n",
    "    factory = ModelFactory()\n",
    "    model = factory.get_model(\n",
    "        model_name='multitask_nn',\n",
    "        input_size=X_small.shape[1],\n",
    "        output_size=Y_small.shape[1],\n",
    "        config=cfg,\n",
    "    )\n",
    "\n",
    "    run_dir = grid_root / f\"run_{run_idx:02d}\"\n",
    "    metrics = run_train_test_split(\n",
    "        model=model,\n",
    "        X=X_small,\n",
    "        Y=Y_small,\n",
    "        test_size=cfg['preprocessing']['test_size'],\n",
    "        output_dir=run_dir,\n",
    "        config_meta={\n",
    "            'selected_features': selected_features,\n",
    "            'config': cfg,\n",
    "            'cohorts': selected_cohorts,\n",
    "            'search_params': params,\n",
    "        },\n",
    "        random_state=cfg['preprocessing']['random_state'],\n",
    "        label=f\"multitask_nn_run_{run_idx:02d}\",\n",
    "    )\n",
    "\n",
    "    mean_metrics = metrics.mean()\n",
    "    grid_rows.append({\n",
    "        'run': run_idx,\n",
    "        **params,\n",
    "        **mean_metrics.to_dict(),\n",
    "    })\n",
    "\n",
    "grid_df = pd.DataFrame(grid_rows).set_index('run')\n",
    "grid_df.sort_values('f1', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DNA_to_RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
